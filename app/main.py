#General
from langchain.schema import HumanMessage
from dotenv import load_dotenv
import os
from langchain.chat_models import init_chat_model

# for conversation history
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

#for cache
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

class DebugInMemoryCache(InMemoryCache):
    def lookup(self, prompt: str, llm_string: str):
        result = super().lookup(prompt, llm_string)
        if result:
            print("âœ… Cevap CACHE aracÄ±lÄ±ÄŸÄ± ile Ã¼retildi.")
        else:
            print("âŒ Cevap LLM aracÄ±lÄ±ÄŸÄ± ile Ã¼retildi. ")
        return result


def main():
    # ChatGroq modelini baÅŸlat
    model = init_chat_model("llama3-8b-8192", model_provider="groq")

    # Memory oluÅŸtur (sadece konuÅŸma geÃ§miÅŸi saklar)
    memory = ConversationBufferMemory()

    # KonuÅŸma zinciri oluÅŸtur
    conversation = ConversationChain(
        llm=model,
        memory=memory,
        verbose = True
    )

    while True:

        # KullanÄ±cÄ±dan mesaj al
        user_message = input("MesajÄ±nÄ±z: ")

        if user_message == 'Ã§Ä±k':
            break

        # Modelden cevap al - Bu kÄ±sÄ±m Ã§Ä±kartÄ±ldÄ±. Ã‡Ã¼nkÃ¼ history tutma eklendi. Bu ÅŸekilde invoke yapÄ±lÄ±nca tutulmuyordu.
        response = model.invoke([HumanMessage(content=str(user_message))])

        # KullanÄ±cÄ±dan gelen mesajlarÄ± zincire gÃ¶nder
        #response = conversation.predict(input=user_message)
        print("Chatbot:", response)



   

if __name__ == '__main__':
    print('Project is started.')

    # .env'den deÄŸiÅŸkenleri yÃ¼kle
    load_dotenv()

    # API key'i oku
    groq_api_key = os.getenv("GROQ_API_KEY")
    
    # ğŸ” CACHE yapÄ±landÄ±rmasÄ±
    set_llm_cache(DebugInMemoryCache())

    main()
