#General
from langchain.schema import HumanMessage
from dotenv import load_dotenv
import os
from langchain.chat_models import init_chat_model

# for conversation history
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.memory.chat_message_histories import RedisChatMessageHistory

#for cache
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain.cache import SQLiteCache



from langchain.agents import initialize_agent, AgentType
from tool import tool_get_system_time
from langchain_community.tools import DuckDuckGoSearchRun


from langchain.cache import SQLiteCache

class DebugSQLiteCache(SQLiteCache):
    def lookup(self, prompt: str, llm_string: str):
        result = super().lookup(prompt, llm_string)
        if result is None:
            print("âŒ Cache MISS")
        else:
            print("âœ… Cache HIT")
        return result

    def update(self, prompt: str, llm_string: str, response):
        print("ğŸ’¾ Storing result in cache")
        return super().update(prompt, llm_string, response)



# Åu anda kapatÄ±ldÄ±. Ã‡Ã¼nkÃ¼ InMemoryCache yerine SQLiteCache yapÄ±sÄ±na geÃ§ildi.
# class DebugInMemoryCache(InMemoryCache):
#     def lookup(self, prompt: str, llm_string: str):
#         result = super().lookup(prompt, llm_string)
#         if result:
#             print("âœ… Cevap CACHE aracÄ±lÄ±ÄŸÄ± ile Ã¼retildi.")
#         else:
#             print("âŒ Cevap CACHE aracÄ±lÄ±ÄŸÄ± ile Ã¼retilmedi. ")
#         return result


def main():
    # ChatGroq modelini baÅŸlat
    model = init_chat_model("llama3-8b-8192", model_provider="groq")

    #Redis ile history tutmak iÃ§in
    history = RedisChatMessageHistory(
    url="redis://localhost:6379",
    session_id="chat-1"
    )

    # Memory oluÅŸtur (sadece konuÅŸma geÃ§miÅŸi saklar)
    memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=history , return_messages=True)
    search = DuckDuckGoSearchRun()
    
    # ğŸ› ï¸ AraÃ§larÄ± tanÄ±mla   
    tools = [tool_get_system_time,search]


    # ğŸ§  Agent baÅŸlat
    agent = initialize_agent(
        tools=tools,
        llm=model,
        memory=memory,
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION  ,
        verbose=True,
    )

    while True:

        # KullanÄ±cÄ±dan mesaj al
        user_message = input("MesajÄ±nÄ±z: ")

        if user_message == 'Ã§Ä±k':
            break

        # Modelden cevap al - Bu kÄ±sÄ±m Ã§Ä±kartÄ±ldÄ±. Ã‡Ã¼nkÃ¼ history tutma eklendi. Bu ÅŸekilde invoke yapÄ±lÄ±nca tutulmuyordu.
        #response = model.invoke([HumanMessage(content=str(user_message))])

        # KullanÄ±cÄ±dan gelen mesajlarÄ± zincire gÃ¶nder
        response = agent.run(input=user_message)
        print("Chatbot:", response)



   

if __name__ == '__main__':
    print('Project is started.')

    # .env'den deÄŸiÅŸkenleri yÃ¼kle
    load_dotenv()

    # API key'i oku
    groq_api_key = os.getenv("GROQ_API_KEY")
    
    # ğŸ” CACHE yapÄ±landÄ±rmasÄ±
    #set_llm_cache(DebugInMemoryCache())  KalÄ±cÄ± cache'e geÃ§ildiÄŸi iÃ§in ÅŸu anda iptal edildi.
    set_llm_cache(DebugSQLiteCache(database_path="../cache/.langchain_cache.db"))

    main()
