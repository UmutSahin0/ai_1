#General
from langchain.schema import HumanMessage
from dotenv import load_dotenv
import os
from langchain.chat_models import init_chat_model

# for conversation history
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

#for cache
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache


from langchain.agents import initialize_agent, AgentType
from tool import tool_get_system_time
from langchain_community.tools import DuckDuckGoSearchRun





class DebugInMemoryCache(InMemoryCache):
    def lookup(self, prompt: str, llm_string: str):
        result = super().lookup(prompt, llm_string)
        if result:
            print("âœ… Cevap CACHE aracÄ±lÄ±ÄŸÄ± ile Ã¼retildi.")
        else:
            print("âŒ Cevap CACHE aracÄ±lÄ±ÄŸÄ± ile Ã¼retilmedi. ")
        return result


def main():
    # ChatGroq modelini baÅŸlat
    model = init_chat_model("llama3-8b-8192", model_provider="groq")

    # Memory oluÅŸtur (sadece konuÅŸma geÃ§miÅŸi saklar)
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    search = DuckDuckGoSearchRun()
    # ğŸ› ï¸ AraÃ§larÄ± tanÄ±mla
    tools = [tool_get_system_time,search]


    # ğŸ§  Agent baÅŸlat
    agent = initialize_agent(
        tools=tools,
        llm=model,
        memory=memory,
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION  ,
        verbose=True,
    )

    while True:

        # KullanÄ±cÄ±dan mesaj al
        user_message = input("MesajÄ±nÄ±z: ")

        if user_message == 'Ã§Ä±k':
            break

        # Modelden cevap al - Bu kÄ±sÄ±m Ã§Ä±kartÄ±ldÄ±. Ã‡Ã¼nkÃ¼ history tutma eklendi. Bu ÅŸekilde invoke yapÄ±lÄ±nca tutulmuyordu.
        #response = model.invoke([HumanMessage(content=str(user_message))])

        # KullanÄ±cÄ±dan gelen mesajlarÄ± zincire gÃ¶nder
        response = agent.run(input=user_message)
        print("Chatbot:", response)



   

if __name__ == '__main__':
    print('Project is started.')

    # .env'den deÄŸiÅŸkenleri yÃ¼kle
    load_dotenv()

    # API key'i oku
    groq_api_key = os.getenv("GROQ_API_KEY")
    
    # ğŸ” CACHE yapÄ±landÄ±rmasÄ±
    set_llm_cache(DebugInMemoryCache())

    main()
